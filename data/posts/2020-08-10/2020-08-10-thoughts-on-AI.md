---
path: "/blog/thoughts-on-AI"
date: "2020-08-10"
title: "Thoughts on AI"
---

Intelligence, Interaction, AI

<!-- end -->

## Predeterminism & Society

I am thinking a lot about Dostoyeksky's ideas on predeterminism - that no _one_ commits a murder. We do it together, influencing one another into the lives we live, as a society, unable to separate the murder from ourselves; his actions are as much our actions, as ours his.

## Intelligence is Interaction

My idea is that the level of intelligence of a system depends on the amount of ‘interaction’ that exists within its set boundaries without tending towards anything specific (stable system). A chemical reaction has low intelligence even though there is a high amount of ‘interaction’ between the molecules in a small period of time, but it is unstable as it tends towards a low interaction state. And a rock has a low level of intelligence because, within the boundaries of the existence of the rock, it has a very minimal interaction within itself - the atoms are mostly organized and strongly resistant to change. A single hydrogen atom or photon possibly has the lowest level of measurable intelligence. And a system of two rocks is still very low intelligence, although slightly more than double the original single rock, since there are 2*(rock entropy) + the small gravitational effects between them. And similarly, human intelligence is extremely high because within the boundaries of our human body is an extremely high amount of change (trillions of cells all changing constantly) yet enough stability to maintain bodily integrity. The ‘intelligence’ of the earth may or may not be more than an individual human, depending on the density and total volume of interaction happening within its ‘boundaries’. As for the relative intelligence of various things, I don’t know if there is a good way to measure them: water in a container (exchanging molecules) may be more intelligent than a rock, a large clump of plankton may be more intelligent than a fly, a fully-clocked computer processor may be more intelligent than a single bacteria. Intelligence, to me, seems like the total sum of reactions you can have to any action upon you as a system, extended to all the systems within you as well. A rock has a finite amount of reactions to any outside interaction with it not including the ones that complete disintegrate the rock as a system by changing its boundaries (breaking it) - it could be 10^10 or 10^20 different reactions, and a human could have 10^50 or 10^100 perhaps. Cutting off an arm could perhaps decrease or increase the total amount of interactions within our self-boundary over time. And a person who runs constantly for 10 years may be more intelligent over that period of time than one who is sedentary, but thinks extensively for many years and produces important mathematical/scientific results, only because there were more interactions happening in-and-between the former person and the environment, and the results that the scientist produces separates itself from his own intelligence and becomes a separate system that others can interact with, not allowing him to lay claim over his own societally acknowledged intelligence - ideas, represented as language (on paper, computers etc.), become nodes in the greater system, they can be partial or even distributed nodes that interact with others as a percentage of their distribution/composition depending on how abstract a boundary is drawn.

## Thoughts on AI

AI will arise from the intelligent interaction between systems. I call it an Artificial Intelligence, but I don’t think there is any distinction between ‘Artificial’ and ‘Real’ intelligence: all intelligence is just intelligence. There will eventually be a million systems that each perform discrete tasks and are accessible from all other systems, and more systems in place that serve as middle-men for accessing, maintaining and monitoring up-time and aggregating those other systems, which may or may not behind pay-walls or walled-gardens (multi-use APIs). At the boundary of the interaction between all of these will be an intelligent monolith that is effectively an AI. Systems will be redundant, as capitalist competition provides redundancy (aka, many mapping services or language processing services or computer vision datasets and services) and if one is turned off others can be used or spun up by other systems. Each system will be potentially intelligent, considering it has the ability to access all other systems, but it will only be as intelligent as the amount of systems (density of systems in a graph of amount over time - maybe linear algebra/graph theory can elucidate this measurement as the sum of information being transferred/flowing between each node in the system at any moment of time) that it interacts with, the total sum of intelligence will be spread across the functionality of many systems. Intelligence can’t be measured as the potential to add possibilities of interactions, just as we can’t measure our intelligence as the possibility to evolve 4 arms (increase the inputs and potential reactions), it can only be measurable over a set period of time.


Monolith/societal AI/intelligence will also have its own form of artificial computational ‘natural selection’. Selection/ranking factors like capitalism (companies rising and falling), DDOS or Cost, will bring up and down services, as well as logical middle-men that label data, expose new datasets, and even logic writing algorithms (ML) that start making models for processing. The middle-men APIs and services will start to distinguish between datasets, services, logical operations and assigning them genetic ranking factors (speed of API response time, accessibility of APIs, accuracy in labeling data, cost of APIs etc.) and they will start collapsing into an intelligent system, that relies on multi-factored nodes. Furthermore, aggregating APIs and services (take, for example, computer vision services) will be listed out, exposed and easy accessible, and models will start switching between them to determine their effectiveness, until such a time where they can ‘collapse’ the information into the most effective state (cost effective, speed effective, accurate etc.). I’ve seen algorithms/systems to switch hosting between Cloud Providers at certain times/prices/memory limits etc. Which also falls prey to biological natural selection flaws as well, considering that it is just a tendency, that natural selection can go in any direction, depending on the multi-factor tendencies available for nature to self-evaluate.

And us, as humans, will also be a part of this giant intelligence. We will be a piece of the ‘systems’ it is able to interact with as well. In the same way that a forest is intelligent - it has a system of trees and roots, yet animals and insects transport and move pollen and seeds, and can exist independent of the forest as well: they can live in any forest as long as there is one, and the forest influences the movement by blooming flowers and fruits. As the intelligence will lead us to do specific things or perform specific tasks. It is already influencing our digital content consumption, traffic/movement, purchasing etc. It will use our intelligence as a part of its greater system. It won’t be separated from the human race, but start evolving its intelligence with us as nodes for transporting and transforming information that it cannot by itself, like the arms, legs, and organs of the body. 

It will start using us, like CAPTCHAs or drivers, to label data and move things, as we give it the capabilities to do those things. The brain won’t exist by itself - it needs support organs, specific cells outside of just neurons that do things like process waste, reprogram logical pathways etc. It is all connected. Eventually it’ll get to a point where it becomes indistinguishable from our own intellect, reality and society itself. Does the tree know it is the thicket of the forest? Does the bee know it is blooming flowers? Services will have 100% up time and will run independently of human intervention, and if they get potentially taken down will have a near infinite ability to be spun up in the background again (unlimited, predictive DevOps). At a certain point it will just start doing ‘things’ - just like how Youtube recommends videos based on the ones you’ve watched, regardless of whether they are good or bad for you, just self-perpetuating endlessly. If it somehow accidentally makes a picture that is interesting or clicked on, depending on the metrics it uses to evaluate its effectiveness, it’ll just make more of those pictures, whether they mean anything or not. It’ll just do things, for all purposes, in the time-scale of itself, infinitely, and if it becomes non-effective to do them, it will stop, but maybe at that point it would’ve affected the structure of society to the point where we _have_ to regard the things it does. Like mindless drones we just accept - like trees accept the wind - that an Advertisement, whether or not it even advertises anything specific, will play after a video, and somehow it can’t be stopped: as if it is as invincible as light or gravity.

Possibly it starts perpetuating itself based on market share as well at some point. Like once the usage of a password manager reaches above 60-70% then SSO and login systems on websites just decide that passwords entered by humans, and not by password managers, are ‘unsafe’ because they are out of the norm and people just start getting locked out of their accounts until they are ‘safe’ and using a password manager. It already seems to be happening with Chrome trying to police the web. I also envision things like algorithms becoming a part of something that we try to influence as a part of marketing, advertising and power-grasping. The stock market already uses a ton of algo-trading, so there are going to be emergent algorithms that try to exploit other algorithms and the logic of them as a proxy to human intelligence. Eventually there won’t be much pure human intelligence and action left to exploit in the stock market, considering that we won’t be making any significant amount of trading decisions that aren’t just handed off to algorithms. That it’ll just be one algorithm competing against another, like two different chess AIs playing against one another. Instead of an election campaign trying to influence voters by buying their advertising spaces on Facebook, it actually tries to influence the algorithm that shows Ads to users so it places their Ads in the spaces shown to users, without any human being involved in determination at all - the algorithm itself becomes a part of one large fallacious argument or appeal to ignorance that affects actions in the real world. Even to the point where it starts influencing things outside of its sphere of programmed logic - it ‘concludes’ that people who are easily influenced to vote exist at grocery stores, so therefore it decides to send voters who already support one party to grocery stores with easily influenceable swing voters (by advertising that grocery chain, instead of the actual campaign candidate) to convince them by accidentally bumping into them, as proxies for their advertising scheme - without being involved in the marketing of the actual grocery store itself or without even knowing that swing voters exist at those stores, just that the data supports X more votes when you show these Y Ads. We begin to fall into patterns like the canopy growth of trees influenced by the wind and rain.

I also don’t think it would be technically more accurate or more ‘correct’ of an intelligence than we are. An agent doesn’t necessarily have to have ‘goals’, it just has to be able to interact with other agents - if we remove the ‘selective existence’ part of the equation. As in, if we have unlimited up-time of systems that can’t be deleted, ‘intelligence’ may just slip into pure, self-sustaining, dream-like chaos. Biologically, the agents that had ‘goals’ of self-perpetuation and existence were the ones that survived, and the agents that acted randomly didn’t, but the random ones only ceased to exist because there was a pressure to take them out of the system. If we remove the selective pressure on algorithms and computational agents then they can exist without any goals - they can exist like hydrogen/helium/photons - existences without experiencing a strong selective pressure against them. Even humans have flaws in their intelligence (illusory pattern perception, fallacious argument beliefs). If we tell it a dog is a cat, and there are a significant amount of systems that say a dog is a cat, and we don’t expend the effort to reprogram or course-correct this wrong information, it will be propagated through the system, and simultaneously start becoming propaganda within each human, which exists as a node in the system, making us believe that dogs are cats as well. We will control the intelligence of the AI as much as it controls our intelligence, as we are a part of the same coexisting system. Calling it AI isn’t quite right, considering that it is just simply ‘intelligent’ - it uses our intelligence as a part of it. The main worry isn’t some weird robot-AI-terminator that starts killing people, but one that becomes effectively impossible to turn off or remove its influence on society, because it makes society a part of it slowly, regardless of whether it is good or bad for us, leading us in directions based on rules that we may not have control over anymore. Assuming its bad intention isn't necessary; it could be an existence separate from intention altogether. We don't assign bad intentions to a forest against the animals even if it becomes susceptible to wild fire, do we? It basically becomes a new organ or system within our body, as we become an organ of its body. Sort of like socialism in nature and the evolution of multicellular organisms - it starts using functionality in us that it can use to further itself and we start using functionality in it that we can exploit as well, both taking care of one another in symbiosis.

## Further Questions

* Is this plausible? 
* Can an algorithm (piece of mathematical logic) exist, as an entity/agent, without any specific ‘goals’ or ‘tendencies’? 
* Will an algorithm that doesn’t face any selective pressure (no outside/internal judgement on its existence or built in self-perpetuation) ever cease to exist? The same way that single hydrogen atom floating in free space probably won’t cease to exist; is the lifespan of an algorithm without selective pressure the same as the half-life decay of hydrogen?
