---
path: "/blog/day-1"
date: "2020-10-04"
title: "Day 1: Let's [Organize Reddit]"
---

But, why not.

<!-- end -->

## Day 1

As a person that has gone on Reddit a significant amount--you can get so lost in the different subreddits and trapped in a rabbit hole of never-ending content. In order to reduce the clickability of them I decided to just organize them as I go--and use a script tied to a Google Sheet, Redditmetrics Data, and a keywords list.

## Reddit Scripting

The script runs with 3 args: `python3 reddit-data-wrangling.py 2020-08-28.csv nsfw_subs.csv nsfw_words.csv`
The basic premise being that we don't really need the nsfw stuff, just the spicy memes.

```python3
# Google Sheets API
from __future__ import print_function
import pickle
import os.path
from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request

import csv
import sys
import praw
import pprint
import requests
import re
from bs4 import BeautifulSoup

def janitor():
    '''
    with open(sys.argv[1], encoding='ISO-8859-1') as all_subreddit_list, open(sys.argv[2]) as my_subreddits:
        # Get all attended subs.
        my_subreddits_csv = csv.reader(my_subreddits, delimiter=',')
        next(my_subreddits_csv)
        organized_subs = set()
        for row in my_subreddits_csv:
            [organized_subs.add(sub.lower() if sub[:3] != '/r/' else sub[3:].lower()) for sub in row]
        organized_subs.remove('')

        # Get all subs.
        # Remove subs with < 1000 subscribers.
        # Remove subs that I attend.
        all_subreddit_list_csv = csv.reader(all_subreddit_list, delimiter=',')
        next(all_subreddit_list_csv)
        unorganized_subs = []
        for row in all_subreddit_list_csv:
            if row[0].lower() not in organized_subs and int(row[-1]) > 1000:
                    if any([word in row[0].lower() or word in row[1].lower() for word in EXCLUSION_LIST]):
                        continue
                    unorganized_subs.append(row)  
        unorganized_subs.sort(key=lambda x: int(x[-1]), reverse=True)

        # Write back.
        with open('unorganized_subreddits.csv', mode='w') as unorganized_subreddits_file:
            sub_writer = csv.writer(unorganized_subreddits_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
            for row in unorganized_subs:
                sub_writer.writerow(row)
    '''

    multi_reddits = parse_multireddits()
    #print(multi_reddits)
    cs_resources = parse_cs_resources()
    #print(cs_resources)
    sheet = parse_sheet()
    #print(sheet)
    nsfw, EXCLUSION_LIST = parse_nsfw()
    attended_subreddits = set(multi_reddits + cs_resources + sheet + nsfw)



    # need to make INCLUSION_LIST and separate out subs by inclusion as well at the end if the list exists
    # easy to categorize by name/description and throwing it into a file for eyed organization - search keywords: game, show, news etc.
    # games = ['game', 'valve', 'steam', 'rpg']
    # cooking = ['cook', 'culinary', 'recipe']
    
    with open(sys.argv[1], encoding='ISO-8859-1') as all_subreddit_list:
        # Get all subs.
        # Remove subs with < 1000 subscribers.
        # Remove subs that I attend.
        all_subreddit_list_csv = csv.reader(all_subreddit_list, delimiter=',')
        next(all_subreddit_list_csv)
        unorganized_subs = []
        for row in all_subreddit_list_csv:
            if row[0].lower() not in attended_subreddits and int(row[-1]) > 1000:
                    if any([word in row[0].lower().strip() or word in row[1].lower().strip() for word in EXCLUSION_LIST]):
                        continue
                    unorganized_subs.append(row)
        unorganized_subs.sort(key=lambda x: int(x[-1]), reverse=True)

        # Write back.
        with open('unorganized_subreddits.csv', mode='w') as unorganized_subreddits_file:
            sub_writer = csv.writer(unorganized_subreddits_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
            for row in unorganized_subs:
                sub_writer.writerow(row)

reddit = praw.Reddit(client_id='BAm-BuV11Ztvng', client_secret='d3PWEJme4R9Nx_49WMqsdRZUjYE', user_agent='script')

def parse_multireddits():
    multis = [multi.display_name for multi in reddit.redditor('SECRETS').multireddits()]
    subreddits_in_multis = []
    for multi in multis:
        multi_reddit = reddit.multireddit('SECRETS', multi)
        subreddits_in_multis += [subreddit.display_name for subreddit in multi_reddit.subreddits]
        for line in multi_reddit.description_md.splitlines():
            if '- /r/' in line:
                subreddits_in_multis.append(line.split('- /r/')[1])

    subreddits_in_multis = [sub.lower() for sub in subreddits_in_multis]
    return subreddits_in_multis

def parse_cs_resources():
    link = 'SECRETS'
    html = requests.get(link).text
    soup = BeautifulSoup(html, 'html.parser')
    subreddits = re.findall(r"\[.*?\]", soup.get_text())
    subreddits = [sub[4:-1].lower() for sub in subreddits]
    return subreddits

# Google Sheets API - https://developers.google.com/sheets/api/quickstart/python
# If modifying these scopes, delete the file token.pickle.
SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']
SAMPLE_SPREADSHEET_ID = 'SECRETS'
SAMPLE_RANGE_NAME = 'SECRETS'

def parse_sheet():
    creds = None
    # The file token.pickle stores the user's access and refresh tokens, and is
    # created automatically when the authorization flow completes for the first
    # time.
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    # If there are no (valid) credentials available, let the user log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        # Save the credentials for the next run
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)

    service = build('sheets', 'v4', credentials=creds)

    # Call the Sheets API
    sheet = service.spreadsheets()
    result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,
                                range=SAMPLE_RANGE_NAME).execute()
    values = result.get('values', [])

    if not values:
        print('No data found.')
    else:
        organized_subs = set()
        for row in values:
            [organized_subs.add(sub.lower() if sub[:3] != '/r/' else sub[3:].lower()) for sub in row]
        organized_subs.remove('')
        return list(organized_subs)


def parse_nsfw():
    # https://www.reddit.com/r/nsfw411/wiki/fulllist1
    nsfw = []
    words = []
    with open(sys.argv[2]) as nsfw_sub_list, open(sys.argv[3]) as nsfw_word_list:
        nsfw_subs = csv.reader(nsfw_sub_list, delimiter='|')
        nsfw_words = csv.reader(nsfw_word_list, delimiter=',')
        next(nsfw_subs)
        next(nsfw_subs)
        for row in nsfw_subs:
            nsfw.append(row[1][2:])
        for row in nsfw_words:
            words.append(row[0])
    return nsfw, words


if __name__ == '__main__':
    janitor()

```
